{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  Building Neural Networks with Keras\n",
    "![network](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png)\n",
    "\n",
    "This notebook is an introduction to building neural networks with the [Keras](https://keras.io) API.\n",
    "\n",
    "# Table of contents\n",
    "* [Introduction](#introduction)\n",
    "* [What are neural networks?](#intro-neural-nets)\n",
    "* [Adding regularization](#regularization)\n",
    "* [Computer vision: building convolution neural networks for the MNIST dataset](#cnn-mnist)\n",
    "* [Saving and loading models](#saving)\n",
    "* [Visualizing networks with TensorBoard](#tensorboard)\n",
    "* [Additional resources](#additional-resources)\n",
    "\n",
    "# Introduction <a id=\"introduction\"></a>\n",
    "One of the biggest recent developments in machine learning is the usage of [_**artificial neural networks**_](https://en.wikipedia.org/wiki/Artificial_neural_network) (ANNs) as a powerful tool in the data scientist's toolbox. Although neural nets have been around for over 70 years, they've only become popular in the past decade thanks to advancements in algorithms and hardware.\n",
    "\n",
    "On the software side, [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) (aka backwards-mode automatic differentiation), introduced in the 1980s, made it much easier to train neural networks. Later, researchers introduced neural network architectures such as [convolutional networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNNs) and [LSTMs](https://en.wikipedia.org/wiki/Long_short-term_memory) that saw success in tasks such as handwriting recognition.\n",
    "\n",
    "Meanwhile, hardware limitations originally made it difficult to train neural networks, which require more data and computing power than most machine learning methods. However, improvements in processor power and the advent of specialized hardware such as graphics processing units have made training neural nets much faster. Massively parallel computing has also enabled boosts in training time: for instance, Google's [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero) was trained on 5,000 tensor processing units in parallel in just a few hours.\n",
    "\n",
    "In this workshop, we will be using [Keras](https://keras.io) to build our own neural networks. Keras is a high-level interface for building ANNs, with libraries like [TensorFlow](https://www.tensorflow.org) running on the backend.\n",
    "\n",
    "### Note: you will need to run the following code cell every time you restart this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7c95cc144a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m    \u001b[0;32mas\u001b[0m \u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m   \u001b[0mplotly\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "###\n",
    "### RUN THIS CELL BEFORE USING THE REST OF THE NOTEBOOK\n",
    "###\n",
    "# Set the Keras backend to be TensorFlow\n",
    "import os, shutil\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy             as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import plotly.offline    as py\n",
    "import plotly.graph_objs as go\n",
    "from   plotly import tools\n",
    "py.init_notebook_mode(connected=True)\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.regularizers          import l2\n",
    "from keras.datasets              import mnist\n",
    "from keras                       import backend\n",
    "from keras.utils                 import to_categorical\n",
    "from keras.layers                import Dense, Dropout\n",
    "from keras.layers                import Conv2D, MaxPooling2D, Flatten\n",
    "from keras.models                import Sequential, load_model\n",
    "\n",
    "########## Helper functions\n",
    "def square_axes(ax, data, expansion_factor=1.05):\n",
    "    # Change limits of plot axes to center on the input dataset, and to put the\n",
    "    # x-axis and y-axis on the same scale\n",
    "    m        = np.mean(data)\n",
    "    max_dist = max([np.linalg.norm(u-m) for u in data]) * expansion_factor\n",
    "    lims     = [m-max_dist, m+max_dist]\n",
    "    try:    ax.set_xlim(lims); ax.set_ylim(lims)\n",
    "    except: ax.xlim(lims); ax.ylim(lims)\n",
    "\n",
    "def plot_decision_boundary(X, clf, ax, incr=1, h=.02):\n",
    "    # Plot the decision boundary for a machine learning classifier on 2D data\n",
    "    xmin, xmax = X[:,0].min()-incr, X[:,0].max()+incr\n",
    "    ymin, ymax = X[:,1].min()-incr, X[:,1].max()+incr\n",
    "    xx, yy     = np.meshgrid(np.arange(xmin,xmax,h),np.arange(ymin,ymax,h))\n",
    "    Z          = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=.2)\n",
    "\n",
    "def load_mnist_wrapper(n_classes=10):\n",
    "    # Loads the MNIST dataset, and does some preprocessing\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Only keep the first n_classes digits\n",
    "    X_train = X_train[y_train < n_classes]; y_train = y_train[y_train < n_classes]\n",
    "    X_test  = X_test[y_test < n_classes];   y_test  = y_test[y_test < n_classes]\n",
    "    \n",
    "    # Pre-process the data for the backend we're using\n",
    "    if backend.image_data_format() == 'channels_first':\n",
    "        X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   1, 28, 28)\n",
    "    else:\n",
    "        X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "        X_test  = X_test.reshape(X_test.shape[0],   28, 28, 1)\n",
    "\n",
    "    # Change pixel intensities from the range 0 - 255 to the range 0 - 1\n",
    "    X_train = X_train / 255\n",
    "    X_test  = X_test  / 255\n",
    "\n",
    "    # Convert the class labels into a format that Keras will be able to parse\n",
    "    y_train = to_categorical(y_train, n_classes)\n",
    "    y_test  = to_categorical(y_test,  n_classes)\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def lm_vs_ann_helper(x_train, y_train, x_test, y_test, lm, ann):\n",
    "    \"\"\"Helper function to compare a linear model against a neural net\n",
    "    on 2D data. Used for section on regularization.\"\"\"\n",
    "    print('ANN training loss:          %5.4f' % mean_squared_error(y_train, ann.predict(x_train)))\n",
    "    print('ANN testing loss:           %5.4f' % mean_squared_error(y_test,  ann.predict(x_test)))\n",
    "    print('Linear model training loss: %5.4f' % mean_squared_error(y_train, lm.predict(x_train)))\n",
    "    print('Linear model testing loss:  %5.4f' % mean_squared_error(y_test,  lm.predict(x_test)))\n",
    "\n",
    "    # Create two subplots, one for just showing the data and one for\n",
    "    # showing both data + predictions\n",
    "    fig = tools.make_subplots(rows=1, cols=2, print_grid=False)\n",
    "    \n",
    "    # Show training and testing data\n",
    "    for col in (1,2):\n",
    "        fig.append_trace(go.Scatter(x=x_train.flatten(), y=y_train.flatten(), mode='markers',\n",
    "                                    name='Training data', marker={'color': 'red', 'opacity': .75},\n",
    "                                    showlegend=(col == 1), hoverinfo='none'), row=1, col=col)\n",
    "        fig.append_trace(go.Scatter(x=x_test.flatten(), y=y_test.flatten(), mode='markers',\n",
    "                                    name='Testing data', marker={'color': 'blue', 'opacity': .75},\n",
    "                                    showlegend=(col == 1), hoverinfo='none'), row=1, col=col)   \n",
    "    # Show predictions\n",
    "    xx = np.linspace(x.min(), x.max(), num=100).reshape((-1,1))\n",
    "    fig.append_trace(go.Scatter(x=xx.flatten(), y=ann.predict(xx).flatten(), mode='lines',\n",
    "                                name='Neural network', hoverinfo='none', line={'width': 3}),\n",
    "                     row=1, col=2)\n",
    "    fig.append_trace(go.Scatter(x=xx.flatten(), y=lm.predict(xx).flatten(), mode='lines',\n",
    "                                name='Linear regression', hoverinfo='none',\n",
    "                                line={'width': 3, 'dash': 'dash', 'color': 'black'}),\n",
    "                     row=1, col=2)\n",
    "    # Make the plots a little easier to see\n",
    "    fig['layout'].update(height=400, width=1000,\n",
    "                         margin={'l':0,'r':0,'t':0,'b':0})\n",
    "    py.iplot(fig)\n",
    "##########################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "# Pre-load data\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = load_mnist_wrapper(n_classes=\n",
    "                                                                                  n_mnist_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are neural networks? <a id=\"intro-neural-nets\"></a>\n",
    "There are many ways to conceptualize neural networks. To me, the most useful way is to think of a neural net as a chain of computers, known as *neurons* in the neural net lingo. The first neuron/computer takes in some data from the outside environment, and the last one spits out a prediction about that data. Along the chain, each neuron takes some data from the previous neuron in the chain, transforms it (by applying an _activation function_ that is chosen by the maker of the neural net), and then passes it on to the next neuron. Training a neural network means figuring out how it should give the results of one step in the chain to the next step.\n",
    "\n",
    "In general, each step in the chain is actually a layer of multiple neurons, who all perform some transformations and give their outputs to every neuron in the next layer.\n",
    "\n",
    "<p style=\"font-size: 11px\"><em><a href=\"https://commons.wikimedia.org/wiki/File:Colored_neural_network.svg#/media/File:Colored_neural_network.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png\" alt=\"Colored neural network.svg\" style=\"width: 40%; height: 40%\"></a><br>By <a href=\"//commons.wikimedia.org/wiki/User_talk:Glosser.ca\" title=\"User talk:Glosser.ca\">Glosser.ca</a> - <span class=\"int-own-work\" lang=\"en\">Own work</span>, Derivative of <a href=\"//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg\" title=\"File:Artificial neural network.svg\">File:Artificial neural network.svg</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=24913461\">Link</a></em></p>\n",
    "\n",
    "The way in which the outputs from one layer are given as inputs to the next layer are determined by a set of _weights_ -- numbers that tell us, for instance, how important the output of neuron $i$ in layer $k$ is to neuron $j$ in layer $k+1$. When we train a neural network, we try to minimize a _loss function_ that takes on large values when we make bad predictions and small values when we make good predictions. As we minimize the loss function, we update the weights of the network to make better and better predictions.\n",
    "\n",
    "For the rest of this notebook we'll actually try to create some neural networks with Keras. If you're interested in the theory and training of neural nets, visit the [additional resources section](#additional-resources) at the end of this notebook.\n",
    "\n",
    "We'll build our first neural net by trying to classify poinst in an example dataset. The dataset consists of two circles of points, one inscribed inside the other. The points on the outer circle are in class 0, and the points on the inner circle are in class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD8CAYAAACfF6SlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPEXBEGIIWL8BUEQYsj/Bwgg7WA2VLmWAAAAAElFTkSuQmCC\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (http://matplotlib.org/) -->\n<svg height=\"252pt\" version=\"1.1\" viewBox=\"0 0 383 252\" width=\"383pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 252.018125 \nL 383.982812 252.018125 \nL 383.982812 0 \nL -0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 38.482813 228.14 \nL 373.282813 228.14 \nL 373.282813 10.7 \nL 38.482813 10.7 \nz\n\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mab790c9f36\" style=\"stroke:#8dd3c7;\"/>\n    </defs>\n    <g clip-path=\"url(#peaee0ac565)\">\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"257.268701\" xlink:href=\"#mab790c9f36\" y=\"38.622721\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"329.095231\" xlink:href=\"#mab790c9f36\" y=\"147.06534\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"326.366544\" xlink:href=\"#mab790c9f36\" y=\"79.414024\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"294.107277\" xlink:href=\"#mab790c9f36\" y=\"55.957075\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"111.095952\" xlink:href=\"#mab790c9f36\" y=\"192.927239\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"248.727601\" xlink:href=\"#mab790c9f36\" y=\"188.483187\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"325.04097\" xlink:href=\"#mab790c9f36\" y=\"80.387147\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"159.500027\" xlink:href=\"#mab790c9f36\" y=\"34.15043\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"114.964702\" xlink:href=\"#mab790c9f36\" y=\"57.140107\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"182.887748\" xlink:href=\"#mab790c9f36\" y=\"186.49365\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"104.609354\" xlink:href=\"#mab790c9f36\" y=\"87.519593\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"120.223088\" xlink:href=\"#mab790c9f36\" y=\"154.916425\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"83.968725\" xlink:href=\"#mab790c9f36\" y=\"99.341872\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"281.561338\" xlink:href=\"#mab790c9f36\" y=\"34.091129\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"76.410193\" xlink:href=\"#mab790c9f36\" y=\"170.176275\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"232.9568\" xlink:href=\"#mab790c9f36\" y=\"22.952395\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"324.176585\" xlink:href=\"#mab790c9f36\" y=\"99.100653\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"278.143776\" xlink:href=\"#mab790c9f36\" y=\"188.477865\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"157.778791\" xlink:href=\"#mab790c9f36\" y=\"193.615279\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"195.567882\" xlink:href=\"#mab790c9f36\" y=\"30.828525\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"134.289874\" xlink:href=\"#mab790c9f36\" y=\"173.889246\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"332.501727\" xlink:href=\"#mab790c9f36\" y=\"83.849066\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"210.959567\" xlink:href=\"#mab790c9f36\" y=\"212.764897\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"161.750093\" xlink:href=\"#mab790c9f36\" y=\"199.064848\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"356.936691\" xlink:href=\"#mab790c9f36\" y=\"134.489235\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"225.205273\" xlink:href=\"#mab790c9f36\" y=\"32.798926\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"163.282777\" xlink:href=\"#mab790c9f36\" y=\"45.113558\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"95.9731\" xlink:href=\"#mab790c9f36\" y=\"65.940477\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"92.095442\" xlink:href=\"#mab790c9f36\" y=\"147.087701\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"326.51032\" xlink:href=\"#mab790c9f36\" y=\"126.691699\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"268.895403\" xlink:href=\"#mab790c9f36\" y=\"39.509955\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"296.269293\" xlink:href=\"#mab790c9f36\" y=\"156.917846\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"83.779254\" xlink:href=\"#mab790c9f36\" y=\"134.366343\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"103.792856\" xlink:href=\"#mab790c9f36\" y=\"125.944356\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"284.907653\" xlink:href=\"#mab790c9f36\" y=\"175.756215\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"303.620198\" xlink:href=\"#mab790c9f36\" y=\"146.91611\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"307.286796\" xlink:href=\"#mab790c9f36\" y=\"162.118746\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"295.996417\" xlink:href=\"#mab790c9f36\" y=\"50.85849\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"181.976243\" xlink:href=\"#mab790c9f36\" y=\"22.500889\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"54.828934\" xlink:href=\"#mab790c9f36\" y=\"97.386554\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"124.991082\" xlink:href=\"#mab790c9f36\" y=\"159.221865\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"323.675856\" xlink:href=\"#mab790c9f36\" y=\"109.161964\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"92.896962\" xlink:href=\"#mab790c9f36\" y=\"163.224702\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"127.764189\" xlink:href=\"#mab790c9f36\" y=\"85.855926\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"322.876377\" xlink:href=\"#mab790c9f36\" y=\"56.765273\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"195.000345\" xlink:href=\"#mab790c9f36\" y=\"200.386284\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"312.868355\" xlink:href=\"#mab790c9f36\" y=\"145.0758\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"96.071294\" xlink:href=\"#mab790c9f36\" y=\"102.243535\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"343.47506\" xlink:href=\"#mab790c9f36\" y=\"120.830021\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"348.721823\" xlink:href=\"#mab790c9f36\" y=\"122.921832\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"330.520657\" xlink:href=\"#mab790c9f36\" y=\"148.375826\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"122.067744\" xlink:href=\"#mab790c9f36\" y=\"174.026958\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"95.199438\" xlink:href=\"#mab790c9f36\" y=\"80.481775\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"162.290202\" xlink:href=\"#mab790c9f36\" y=\"202.152092\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"89.641094\" xlink:href=\"#mab790c9f36\" y=\"106.694671\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"68.070651\" xlink:href=\"#mab790c9f36\" y=\"92.481251\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"296.326393\" xlink:href=\"#mab790c9f36\" y=\"194.792471\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"298.576577\" xlink:href=\"#mab790c9f36\" y=\"55.324609\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"190.600683\" xlink:href=\"#mab790c9f36\" y=\"28.166555\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"233.093024\" xlink:href=\"#mab790c9f36\" y=\"45.95957\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"279.368486\" xlink:href=\"#mab790c9f36\" y=\"28.93051\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"252.446246\" xlink:href=\"#mab790c9f36\" y=\"212.975507\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"97.378477\" xlink:href=\"#mab790c9f36\" y=\"128.038433\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"205.906915\" xlink:href=\"#mab790c9f36\" y=\"203.224576\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"314.723113\" xlink:href=\"#mab790c9f36\" y=\"109.282748\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"331.679502\" xlink:href=\"#mab790c9f36\" y=\"68.647141\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"159.095482\" xlink:href=\"#mab790c9f36\" y=\"210.445051\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"250.070033\" xlink:href=\"#mab790c9f36\" y=\"189.901541\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"305.718143\" xlink:href=\"#mab790c9f36\" y=\"170.990254\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"293.904542\" xlink:href=\"#mab790c9f36\" y=\"50.586809\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"136.240495\" xlink:href=\"#mab790c9f36\" y=\"34.837388\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"65.422787\" xlink:href=\"#mab790c9f36\" y=\"152.759304\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"144.352653\" xlink:href=\"#mab790c9f36\" y=\"188.83981\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"339.446216\" xlink:href=\"#mab790c9f36\" y=\"104.573504\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"104.170816\" xlink:href=\"#mab790c9f36\" y=\"64.472638\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"267.362788\" xlink:href=\"#mab790c9f36\" y=\"48.086364\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"114.36571\" xlink:href=\"#mab790c9f36\" y=\"60.041937\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"69.224467\" xlink:href=\"#mab790c9f36\" y=\"138.307144\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"155.982578\" xlink:href=\"#mab790c9f36\" y=\"28.546283\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"189.224091\" xlink:href=\"#mab790c9f36\" y=\"28.061473\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"176.023233\" xlink:href=\"#mab790c9f36\" y=\"32.061337\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"143.212136\" xlink:href=\"#mab790c9f36\" y=\"34.174792\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"180.635832\" xlink:href=\"#mab790c9f36\" y=\"21.823726\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"317.482297\" xlink:href=\"#mab790c9f36\" y=\"117.380592\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"249.550245\" xlink:href=\"#mab790c9f36\" y=\"195.600063\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"255.488522\" xlink:href=\"#mab790c9f36\" y=\"217.016274\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"182.863428\" xlink:href=\"#mab790c9f36\" y=\"216.490339\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"233.107081\" xlink:href=\"#mab790c9f36\" y=\"213.486445\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"184.649216\" xlink:href=\"#mab790c9f36\" y=\"24.348518\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"244.498167\" xlink:href=\"#mab790c9f36\" y=\"205.856883\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"114.762236\" xlink:href=\"#mab790c9f36\" y=\"52.848362\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"124.097585\" xlink:href=\"#mab790c9f36\" y=\"172.739244\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"314.852983\" xlink:href=\"#mab790c9f36\" y=\"173.579665\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"165.074251\" xlink:href=\"#mab790c9f36\" y=\"202.896916\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"295.568901\" xlink:href=\"#mab790c9f36\" y=\"179.200487\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"131.871459\" xlink:href=\"#mab790c9f36\" y=\"43.13569\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"246.81181\" xlink:href=\"#mab790c9f36\" y=\"44.246724\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"311.204624\" xlink:href=\"#mab790c9f36\" y=\"70.283064\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"146.892043\" xlink:href=\"#mab790c9f36\" y=\"193.404042\"/>\n     <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"97.504444\" xlink:href=\"#mab790c9f36\" y=\"97.707353\"/>\n    </g>\n   </g>\n   <g id=\"PathCollection_2\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mfcaafd1ab6\" style=\"stroke:#feffb3;\"/>\n    </defs>\n    <g clip-path=\"url(#peaee0ac565)\">\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"201.42732\" xlink:href=\"#mfcaafd1ab6\" y=\"157.292355\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"232.456305\" xlink:href=\"#mfcaafd1ab6\" y=\"106.126\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"214.686737\" xlink:href=\"#mfcaafd1ab6\" y=\"148.304619\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"231.959806\" xlink:href=\"#mfcaafd1ab6\" y=\"146.094036\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"172.954793\" xlink:href=\"#mfcaafd1ab6\" y=\"109.02578\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"181.711888\" xlink:href=\"#mfcaafd1ab6\" y=\"126.700779\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"196.86169\" xlink:href=\"#mfcaafd1ab6\" y=\"94.221756\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"219.711961\" xlink:href=\"#mfcaafd1ab6\" y=\"153.653328\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"191.899356\" xlink:href=\"#mfcaafd1ab6\" y=\"108.327264\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"180.053514\" xlink:href=\"#mfcaafd1ab6\" y=\"88.452515\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"241.126155\" xlink:href=\"#mfcaafd1ab6\" y=\"140.463242\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"174.738495\" xlink:href=\"#mfcaafd1ab6\" y=\"105.813904\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"176.082105\" xlink:href=\"#mfcaafd1ab6\" y=\"115.849105\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"211.323709\" xlink:href=\"#mfcaafd1ab6\" y=\"73.51648\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"230.451468\" xlink:href=\"#mfcaafd1ab6\" y=\"143.520745\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"227.276577\" xlink:href=\"#mfcaafd1ab6\" y=\"142.89178\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"199.750487\" xlink:href=\"#mfcaafd1ab6\" y=\"79.863684\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"163.737386\" xlink:href=\"#mfcaafd1ab6\" y=\"110.19032\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"234.410632\" xlink:href=\"#mfcaafd1ab6\" y=\"92.768312\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"229.110699\" xlink:href=\"#mfcaafd1ab6\" y=\"129.441759\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"210.932712\" xlink:href=\"#mfcaafd1ab6\" y=\"78.609354\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"230.010795\" xlink:href=\"#mfcaafd1ab6\" y=\"98.255423\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"196.477642\" xlink:href=\"#mfcaafd1ab6\" y=\"101.584477\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"240.425537\" xlink:href=\"#mfcaafd1ab6\" y=\"102.605462\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"192.429872\" xlink:href=\"#mfcaafd1ab6\" y=\"142.291141\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"226.563656\" xlink:href=\"#mfcaafd1ab6\" y=\"104.195446\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"193.066214\" xlink:href=\"#mfcaafd1ab6\" y=\"101.576754\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"225.200703\" xlink:href=\"#mfcaafd1ab6\" y=\"90.722584\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"253.130926\" xlink:href=\"#mfcaafd1ab6\" y=\"139.89304\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"187.500605\" xlink:href=\"#mfcaafd1ab6\" y=\"114.901418\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"157.4521\" xlink:href=\"#mfcaafd1ab6\" y=\"137.917468\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"172.08041\" xlink:href=\"#mfcaafd1ab6\" y=\"83.929471\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"208.653107\" xlink:href=\"#mfcaafd1ab6\" y=\"156.679873\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"169.99832\" xlink:href=\"#mfcaafd1ab6\" y=\"116.82409\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"196.634187\" xlink:href=\"#mfcaafd1ab6\" y=\"131.026237\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"177.570869\" xlink:href=\"#mfcaafd1ab6\" y=\"103.079637\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"146.148542\" xlink:href=\"#mfcaafd1ab6\" y=\"104.734677\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"214.156205\" xlink:href=\"#mfcaafd1ab6\" y=\"104.739455\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"172.0266\" xlink:href=\"#mfcaafd1ab6\" y=\"96.435928\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"244.857789\" xlink:href=\"#mfcaafd1ab6\" y=\"107.892185\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"176.957714\" xlink:href=\"#mfcaafd1ab6\" y=\"85.60457\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"248.576862\" xlink:href=\"#mfcaafd1ab6\" y=\"136.925653\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"181.057821\" xlink:href=\"#mfcaafd1ab6\" y=\"109.765022\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"249.377127\" xlink:href=\"#mfcaafd1ab6\" y=\"111.822588\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"240.680117\" xlink:href=\"#mfcaafd1ab6\" y=\"105.072223\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"189.512242\" xlink:href=\"#mfcaafd1ab6\" y=\"93.622594\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"244.960665\" xlink:href=\"#mfcaafd1ab6\" y=\"129.313413\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"245.194625\" xlink:href=\"#mfcaafd1ab6\" y=\"140.692151\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"249.435323\" xlink:href=\"#mfcaafd1ab6\" y=\"130.400631\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"216.130696\" xlink:href=\"#mfcaafd1ab6\" y=\"142.4371\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"212.957328\" xlink:href=\"#mfcaafd1ab6\" y=\"150.28349\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"228.848305\" xlink:href=\"#mfcaafd1ab6\" y=\"93.154092\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"235.470297\" xlink:href=\"#mfcaafd1ab6\" y=\"107.197972\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"185.407431\" xlink:href=\"#mfcaafd1ab6\" y=\"150.567865\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"243.29992\" xlink:href=\"#mfcaafd1ab6\" y=\"120.189388\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"248.121825\" xlink:href=\"#mfcaafd1ab6\" y=\"82.107937\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"196.492048\" xlink:href=\"#mfcaafd1ab6\" y=\"140.229143\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"199.767778\" xlink:href=\"#mfcaafd1ab6\" y=\"145.828776\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"240.924467\" xlink:href=\"#mfcaafd1ab6\" y=\"130.519254\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"193.650264\" xlink:href=\"#mfcaafd1ab6\" y=\"95.205089\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"236.620929\" xlink:href=\"#mfcaafd1ab6\" y=\"116.473832\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"231.696559\" xlink:href=\"#mfcaafd1ab6\" y=\"130.567346\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"243.101274\" xlink:href=\"#mfcaafd1ab6\" y=\"88.094079\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"161.910127\" xlink:href=\"#mfcaafd1ab6\" y=\"136.983132\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"243.359307\" xlink:href=\"#mfcaafd1ab6\" y=\"112.596277\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"224.288671\" xlink:href=\"#mfcaafd1ab6\" y=\"96.827882\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"184.164233\" xlink:href=\"#mfcaafd1ab6\" y=\"111.172086\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"193.000313\" xlink:href=\"#mfcaafd1ab6\" y=\"125.262397\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"163.043234\" xlink:href=\"#mfcaafd1ab6\" y=\"113.030324\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"252.213945\" xlink:href=\"#mfcaafd1ab6\" y=\"117.665016\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"204.944364\" xlink:href=\"#mfcaafd1ab6\" y=\"81.578252\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"243.659432\" xlink:href=\"#mfcaafd1ab6\" y=\"107.030702\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"230.643455\" xlink:href=\"#mfcaafd1ab6\" y=\"72.115535\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"173.397571\" xlink:href=\"#mfcaafd1ab6\" y=\"139.101541\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"241.103749\" xlink:href=\"#mfcaafd1ab6\" y=\"124.885298\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"194.380561\" xlink:href=\"#mfcaafd1ab6\" y=\"95.832839\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"205.650304\" xlink:href=\"#mfcaafd1ab6\" y=\"163.629474\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"213.641635\" xlink:href=\"#mfcaafd1ab6\" y=\"90.166906\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"181.422296\" xlink:href=\"#mfcaafd1ab6\" y=\"128.409819\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"194.186668\" xlink:href=\"#mfcaafd1ab6\" y=\"72.806768\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"249.162529\" xlink:href=\"#mfcaafd1ab6\" y=\"99.913306\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"188.302897\" xlink:href=\"#mfcaafd1ab6\" y=\"119.605512\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"185.179723\" xlink:href=\"#mfcaafd1ab6\" y=\"132.326356\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"218.385812\" xlink:href=\"#mfcaafd1ab6\" y=\"110.099633\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"193.856831\" xlink:href=\"#mfcaafd1ab6\" y=\"130.236075\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"162.085357\" xlink:href=\"#mfcaafd1ab6\" y=\"94.666069\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"191.930989\" xlink:href=\"#mfcaafd1ab6\" y=\"147.723704\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"177.611492\" xlink:href=\"#mfcaafd1ab6\" y=\"125.546291\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"241.016351\" xlink:href=\"#mfcaafd1ab6\" y=\"96.887329\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"170.146781\" xlink:href=\"#mfcaafd1ab6\" y=\"104.064583\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"209.498491\" xlink:href=\"#mfcaafd1ab6\" y=\"110.890107\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"188.160771\" xlink:href=\"#mfcaafd1ab6\" y=\"137.263741\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"234.644066\" xlink:href=\"#mfcaafd1ab6\" y=\"130.672643\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"228.920752\" xlink:href=\"#mfcaafd1ab6\" y=\"154.594293\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"238.316569\" xlink:href=\"#mfcaafd1ab6\" y=\"123.849853\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"196.284842\" xlink:href=\"#mfcaafd1ab6\" y=\"111.939376\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"241.730722\" xlink:href=\"#mfcaafd1ab6\" y=\"117.099897\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"219.603997\" xlink:href=\"#mfcaafd1ab6\" y=\"135.877266\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"236.513043\" xlink:href=\"#mfcaafd1ab6\" y=\"91.747315\"/>\n     <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"194.358902\" xlink:href=\"#mfcaafd1ab6\" y=\"137.186328\"/>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc0a7e469b0\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"83.949272\" xlink:href=\"#mc0a7e469b0\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- −1.0 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-2212\"/>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-31\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-2e\"/>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-30\"/>\n      </defs>\n      <g style=\"fill:#ffffff;\" transform=\"translate(71.807865 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"146.888321\" xlink:href=\"#mc0a7e469b0\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- −0.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-35\"/>\n      </defs>\n      <g style=\"fill:#ffffff;\" transform=\"translate(134.746915 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"209.827371\" xlink:href=\"#mc0a7e469b0\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(201.875809 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"272.766421\" xlink:href=\"#mc0a7e469b0\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(264.814858 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"335.705471\" xlink:href=\"#mc0a7e469b0\" y=\"228.14\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(327.753908 242.738437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m50c89365c3\" style=\"stroke:#ffffff;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m50c89365c3\" y=\"206.417046\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- −1.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 210.216265)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m50c89365c3\" y=\"161.476186\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- −0.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(7.2 165.275404)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-2212\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"179.199219\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m50c89365c3\" y=\"116.535325\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(15.579688 120.334544)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m50c89365c3\" y=\"71.594465\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.5 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(15.579688 75.393684)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"fill:#ffffff;stroke:#ffffff;stroke-width:0.8;\" x=\"38.482813\" xlink:href=\"#m50c89365c3\" y=\"26.653605\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 1.0 -->\n      <g style=\"fill:#ffffff;\" transform=\"translate(15.579688 30.452824)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 38.482813 228.14 \nL 38.482813 10.7 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 373.282813 228.14 \nL 373.282813 10.7 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 38.482812 228.14 \nL 373.282812 228.14 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 38.482812 10.7 \nL 373.282812 10.7 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 298.434375 48.05625 \nL 366.282813 48.05625 \nQ 368.282813 48.05625 368.282813 46.05625 \nL 368.282813 17.7 \nQ 368.282813 15.7 366.282813 15.7 \nL 298.434375 15.7 \nQ 296.434375 15.7 296.434375 17.7 \nL 296.434375 46.05625 \nQ 296.434375 48.05625 298.434375 48.05625 \nz\n\" style=\"opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"PathCollection_3\">\n     <g>\n      <use style=\"fill:#8dd3c7;stroke:#8dd3c7;\" x=\"310.434375\" xlink:href=\"#mab790c9f36\" y=\"24.673437\"/>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- Class 0 -->\n     <defs>\n      <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-43\"/>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-6c\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-61\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path id=\"DejaVuSans-20\"/>\n     </defs>\n     <g style=\"fill:#ffffff;\" transform=\"translate(328.434375 27.298437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"97.607422\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"158.886719\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"210.986328\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"263.085938\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"294.873047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_4\">\n     <g>\n      <use style=\"fill:#feffb3;stroke:#feffb3;\" x=\"310.434375\" xlink:href=\"#mfcaafd1ab6\" y=\"39.351562\"/>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- Class 1 -->\n     <g style=\"fill:#ffffff;\" transform=\"translate(328.434375 41.976562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-43\"/>\n      <use x=\"69.824219\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"97.607422\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"158.886719\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"210.986328\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"263.085938\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"294.873047\" xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"peaee0ac565\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"38.482813\" y=\"10.7\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": "<matplotlib.figure.Figure at 0x10ea8a630>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "plt.scatter(X[classes == 0,0], X[classes == 0,1], label=\"Class 0\")\n",
    "plt.scatter(X[classes == 1,0], X[classes == 1,1], label=\"Class 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will consist of an input layer, one hidden layer, and an output layer. Since our data are two-dimensional, we will have two neurons in the input layer (one for $x$ coordinates, another for $y$ coordinates). We'll put $16$ neurons in the hidden layer, and one neuron in the output layer. The output of the network will be an estimated probability (in the range $[0,1]$) that the data point is in class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c67ef3e69a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of repeatedly calling `model.add()`, you can give Keras the layers you want to have in your neural net when you call `Sequential()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1,  activation='sigmoid')\n",
    "]\n",
    "\n",
    "model = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the network! Our first step is to call `model.compile`. The `compile` function allows us to make some configurations before we start training. We can do things like\n",
    "\n",
    "* Choose a [loss function](https://keras.io/losses/), which helps us score how good our neural net is\n",
    "* Set which [optimizer](https://keras.io/optimizers/) we'll use to minimize the loss function\n",
    "* Set options that will be used on the backend (in this case, by TensorFlow) beneath the Keras API.\n",
    "\n",
    "For more information on compilation options, see the [getting started guide](https://keras.io/getting-started/sequential-model-guide/#compilation) or the [Keras documentation](https://keras.io/models/model/#compile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Options:\n",
    "    loss='binary_crossentropy'\n",
    "        - Loss function for classification problems with two classes.\n",
    "        - If we had more than one class, we'd use 'categorical_crossentropy' instead\n",
    "\n",
    "    optimizer='rmsprop'\n",
    "        - Tells Keras to use the RMSProp optimization routine\n",
    "        - Reference: https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "    metrics=['accuracy']\n",
    "        - When we're training our neural net, Keras will tell us how accurate the net\n",
    "          currently is at making correct predictions.\n",
    "\"\"\"\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been compiled, you can run `model.fit` to actually train the neural network. When you call `model.fit`, you provide a few parameters (such as `epochs` and `steps_per_epoch`) that tell Keras how much training you want to do. In the code below, we do $5$ training epochs each consisting of $512$ training steps, totalling $5\\times 512 = 2560$ steps of neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model.fit takes two numpy arrays. The first is the array X, which contains the actual\n",
    "data. The second is an array y that contains the predictions we are trying to make. Since\n",
    "this is a classification problem with two classes (\"blue\" and \"orange\"), y should contain\n",
    "0's and 1's (where 0 = blue, 1 = orange).\n",
    "\n",
    "Options:\n",
    "    epochs=5\n",
    "        - Each 'epoch' can be thought of as a unit of training, consisting of\n",
    "          many small training steps\n",
    "        - The number of training steps in each epoch is controlled by \n",
    "          steps_per_epoch\n",
    "\n",
    "    steps_per_epoch=512\n",
    "        - Number of training steps to do in each epoch\n",
    "        - The total number of training steps done when model.fit is called is \n",
    "          epochs * steps_per_epoch\n",
    "     \n",
    "    verbose=1\n",
    "        - Tells Keras to show a progress bar for each training epoch that keeps us\n",
    "          updated on how much the network has been trained.\n",
    "\"\"\"\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your neural network has been trained, you can use `model.predict()` to predict which class a point is in. `model.evaluate()` tells you the value of the loss function and the neural network's accuracy on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model.predict\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Determine loss and accuracy with model.evaluate\n",
    "loss, accuracy = model.evaluate(X, classes)\n",
    "\n",
    "print('%-20s %6.5f' % ('Training loss:', loss))\n",
    "print('%-20s %6.5f' % ('Training accuracy:', accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two-dimensional datasets, we can also plot the decision boundary that allows you to see where a machine learning classifier decides which class a point is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plot data, and the decision boundary found by the network\n",
    "\"\"\"\n",
    "fig, axes = plt.subplots(1,2,figsize=(12,6))\n",
    "for ax in axes:\n",
    "    ax.scatter(X[classes == 0,0], X[classes == 0,1])\n",
    "    ax.scatter(X[classes == 1,0], X[classes == 1,1])\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "\n",
    "# plot_decision_boundary is a function I've created that you can\n",
    "# find in the first code cell in this notebook.\n",
    "plot_decision_boundary(X, model, axes[1], incr=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous task is an example of a classification problem: given some data and some labels (e.g. \"inner circle\" and \"outer circle\"), create a neural network that can look at a new point and predict its label. Regression is another common machine learning task. In a regression problem, you must instead predict a *response variable*, which can take on a continuum of values. For instance, all of the following are regression problems:\n",
    "\n",
    "* How well will a student score on a standardized test given their grades in school?\n",
    "* Can we assess how happy a person is (on a scale of 1 - 10) from a sample of their writing?\n",
    "* Given data about air pressure, temperature, and wind speed, can I predict how much rainfall we will receive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-2, 2, size=(80,1))\n",
    "y = np.sin(np.pi * x) + np.random.normal(scale=.15, size=(x.size,1))\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(32, activation='relu', input_dim=1),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(8,  activation='relu'),\n",
    "    Dense(1,  activation='linear')  # Use linear activation in last layer for regression,\n",
    "])                                  # sigmoid or tanh for classification when there's only\n",
    "                                    # one class, and softmax when there's multiple classes\n",
    "\n",
    "# Since we're doing regression, we're going to use 'mean_squared_error' as\n",
    "# our loss function\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "model.fit(x, y, steps_per_epoch=256, epochs=4)\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "xx  = np.linspace(x.min(), x.max(), num=100)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(xx, model.predict(xx), color='r', label='Neural net predictions')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding regularization <a id=\"regularization\"></a>\n",
    "Neural networks are such a power machine learning tool that they can occasionally be a little _too_ powerful. In the [second workshop of this series](https://nbviewer.jupyter.org/github/wshand/Python-Data-Science-Workshop/blob/master/2.%20Intro%20to%20Machine%20Learning%20in%20Python%20with%20Scikit-learn.ipynb), we discussed how a machine learning method can get a great score with the data it was trained on and still fail in the real world. Neural networks also do well with training data, but -- if improperly constructed -- are poor at making predictions for data they've never seen before.\n",
    "\n",
    "As a demonstration, we're going to try another very simple regression problem, like we did in the previous section. However, we're going to use a much more complicated neural net than necessary, consisting of more neurons and layers than we really need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model    import LinearRegression\n",
    "from sklearn.metrics         import mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\"\"\"\n",
    "Create some example data\n",
    "\"\"\"\n",
    "x = np.random.uniform(-1, 1, size=(60,1))\n",
    "y = x + np.random.normal(scale=.15, size=x.shape)\n",
    "\n",
    "# Split into training and testing data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n",
    "\"\"\"\n",
    "Train a complicated neural network on the data\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=1),\n",
    "    Dense(64,  activation='relu'),\n",
    "    Dense(32,  activation='relu'),\n",
    "    Dense(1,   activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "\"\"\"\n",
    "Perform linear regression\n",
    "\"\"\"\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(x_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Compare the two models\n",
    "\"\"\"\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the output I got on one run of the code cell above:\n",
    "\n",
    "> ```\n",
    "Epoch 1/3\n",
    "512/512 [==============================] - 5s 9ms/step - loss: 0.0213\n",
    "Epoch 2/3\n",
    "512/512 [==============================] - 2s 3ms/step - loss: 0.0132\n",
    "Epoch 3/3\n",
    "512/512 [==============================] - 2s 3ms/step - loss: 0.0116\n",
    "ANN training loss:          0.0109\n",
    "ANN testing loss:           0.0386\n",
    "Linear model training loss: 0.0215\n",
    "Linear model testing loss:  0.0188```\n",
    "\n",
    "The mean squared error received by the ANN on the data it was trained with ($0.0102$) was almost four times better than the MSE on the testing data ($0.0386$), which it hadn't seen before. Meanwhile, fitting a line to the same training data via linear regression got an error on the testing data of $0.0188$, less than half of what the ANN got on that data.\n",
    "\n",
    "The problem of getting a much better score on the training data than the testing data is known as *overfitting*. Neural networks can be especially prone to overfitting.\n",
    "\n",
    "There are two ways to prevent overfitting your training data:\n",
    "\n",
    "* make your neural net simpler; or\n",
    "* add regularization to your ANN.\n",
    "\n",
    "If your neural net is doing much better on the training data than the testing data, simplifying your network's architecture is the easiest fix to try. In the code cell below, I try a neural net with only two layers and far fewer neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use a much simpler neural net on this data\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(4, activation='relu', input_dim=1),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern learned by the simplified neural net in this case is much closer to what we got with the linear model. Now we do much better at making predictions for the data we haven't seen before.\n",
    "\n",
    "However, simplifying your network isn't usually the best way to try to prevent overfitting. First, it's difficult to tell what part of the network you need to simplify. Moreover, it decreases the predictive power of your neural net.\n",
    "\n",
    "A better method is to use _regularization_. A regularizer is a method for controlling the growth of the weights in your neural network as you train it. This is useful because in general overfitting occurs due to various oddities that occur to the weights as you train your ANN:\n",
    "\n",
    "* Some weights grow very large. This usually indicates that your neural net learned some strange feature that only occurred by chance via random noise in the training data.\n",
    "* One layer might overemphasize the importance of a few neurons in the previous layers, and ignore the rest of the neurons.\n",
    "\n",
    "To fix the first problem, we change our loss function from\n",
    "\n",
    "$$\n",
    "\\text{loss function} = \\text{numerical penalty on incorrect predictions}\n",
    "$$\n",
    "\n",
    "to\n",
    "\n",
    "$$\n",
    "\\text{loss function} = \\text{numerical penalty on incorrect predictions} + \\text{penalty on size of weights}\n",
    "$$\n",
    "\n",
    "The most common penalties to apply here are known as $L^1$ and $L^2$ regularization. If $\\mathcal{W}$ is the set of all of the weights in the network, then\n",
    "\n",
    "\\begin{align*}\n",
    "L^1 \\text{ regularization:} & & \\text{penalty on size of weights} & = \\lambda\\sum_{w_i\\in\\mathcal{W}} |w_i| \\\\\n",
    "L^2 \\text{ regularization:} & & \\text{penalty on size of weights} & = \\lambda\\sum_{w_i\\in\\mathcal{W}} |w_i|^2\n",
    "\\end{align*}\n",
    "\n",
    "where $\\lambda > 0$ is some number chosen by the designer of the ANN.\n",
    "\n",
    "To fix the second problem, we can use a regularizer known as _dropout_ ([Hinton et al, 2012](https://arxiv.org/abs/1207.0580)). A dropout layer takes the inputs from the previous layer and randomly sets some fraction of them to zero before passing them to the next layer. This prevents the neural net from only relying on a few neurons in each layer -- the outputs of those neurons could be set to zero by a dropout layer and then the ANN wouldn't be able to use their outputs any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers       import Dropout\n",
    "from keras.regularizers import l1\n",
    "\n",
    "# Same neural net as before, but with some added regularization\n",
    "model = Sequential([\n",
    "    # Add some L^2 regularization to keep the weights in the first layer down\n",
    "    Dense(128, activation='relu', input_dim=1, activity_regularizer=l1(1e-5)),\n",
    "    \n",
    "    # Randomly drop 50% of the inputs from the previous layer\n",
    "    Dropout(0.5),\n",
    "    Dense(64,  activation='relu', activity_regularizer=l1(1.6e-4)),\n",
    "    \n",
    "    # Randomly drop 50% of the inputs from the previous layer\n",
    "    Dropout(0.5),\n",
    "    Dense(32,  activation='relu', activity_regularizer=l1(8e-5)),\n",
    "    \n",
    "    # By suggestion of Hinton, don't apply dropout to the output layer\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(x_train, y_train, epochs=3, steps_per_epoch=512, verbose=1)\n",
    "\n",
    "lm_vs_ann_helper(x_train, y_train, x_test, y_test, linear_model, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural net is still overfitting a little bit, but clearly it's doing better than the original version.\n",
    "\n",
    "## Aside: choosing hyperparameters\n",
    "A _hyperparameter_ is a number that must be chosen by the creator of an ML system that affects how the system learns or is designed. Neural networks have an especially large number of hyperparameters -- you have to choose the number of layers, the size of layers, what activation functions should be used in each layer, how much regularization/dropout to apply, and so on.\n",
    "\n",
    "A major downside of neural nets is that an ANN's designer has to adjust each of these parameters until they get a reasonably strong machine learner. The number of hyperparameters can also be beneficial: it allows the designer to make the neural net as powerful as they'd like. But even then, the designer still has to find a way to choose good hyperparameters for the network.\n",
    "\n",
    "* **Suggestions from papers**: the most common way to choose hyperparameters (at least initially) is just to see what experts suggest from past papers. For instance, in his paper introducing dropout, Geoffrey Hinton recommends a dropout rate of $50\\%$. There are also papers such as [\"Practical recommendations for gradient-based training of deep architectures\"](https://arxiv.org/abs/1206.5533v2) by Yoshua Bengio that look at what various heuristics and practical experience say about how you should choose hyperparameters.\n",
    "* **Manual guess-and-check**: it's easy to adjust some of the parameters, re-train your network, and see how much better or worse your ANN is doing. This isn't always a very fast or effective method, though.\n",
    "* **Experience**: in some problem domains there may be specific values of hyperparameters that often work better than others. With enough experience building neural nets for this domain it becomes a lot easier to make good guesses about what hyperparameters you should choose.\n",
    "* **Randomized and grid search**: you can automate the process of hyperparameter selection by choosing some candidate values of each hyperparameter and then iteratively re-training and scoring your neural net on every possible selection of parameters. This is known as _grid search_. You can also just choose some random group of the candidate values, train the ANN, and repeat a few times. This is called _randomized search_. Below I've written some code that uses scikit-learn's `RandomizedSearchCV` class to find good hyperparameters for the regression problem in the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A scikit-learn class that helps us choose hyperparameters\n",
    "from sklearn.model_selection     import RandomizedSearchCV\n",
    "\n",
    "# Wraps around a neural network so that we can use the scikit-learn API\n",
    "# to perform grid search.\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "def create_model(l1_1=0, l1_2=0, l1_3=0):\n",
    "    # Clear past TensorFlow sessions to prevent GridSearchCV from using too much memory\n",
    "    # Reference: https://stackoverflow.com/a/42047606\n",
    "    if backend.backend() == 'tensorflow':\n",
    "        backend.clear_session()\n",
    "    \n",
    "    # Same neural net as before, but with some added regularization\n",
    "    model = Sequential([\n",
    "        # Add some L^1 regularization to keep the weights in the first layer down\n",
    "        Dense(128, activation='relu', input_dim=1, activity_regularizer=l1(l1_1)),\n",
    "\n",
    "        # Randomly drop 50% of the inputs from the previous layer\n",
    "        Dropout(0.5),\n",
    "        Dense(64,  activation='relu', activity_regularizer=l1(l1_2)),\n",
    "\n",
    "        # Randomly drop 50% of the inputs from the previous layer\n",
    "        Dropout(0.5),\n",
    "        Dense(32,  activation='relu', activity_regularizer=l1(l1_3)),\n",
    "\n",
    "        # By suggestion of Hinton, don't apply dropout to the output layer\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "clf = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "l1_params = [2**n * 1e-5 for n in range(8)]\n",
    "param_grid = dict(l1_1=l1_params, l1_2=l1_params, l1_3=l1_params)\n",
    "\n",
    "# Apply grid search\n",
    "grid = RandomizedSearchCV(clf, param_distributions=param_grid, n_jobs=4, cv=3,\n",
    "                          verbose=1, n_iter=30)\n",
    "grid.fit(x,y)\n",
    "\n",
    "# What were the best hyperparameters that we found?\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer vision: building convolutional networks for the MNIST dataset <a id=\"cnn-mnist\"></a>\n",
    "Let's start looking at how neural networks are used for analyzing image data. We're going to use the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database), a classic computer vision dataset containing images of tens of thousands of handwritten digits. The MNIST dataset can actually be loaded into Python using Keras:\n",
    "\n",
    "```python\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "```\n",
    "\n",
    "We will build a neural network that will be able to look at an MNIST digit tell us what number it sees. To help shorten training times we're only going to look at the digits $0$, $1$, and $2$; you can build a network that classifies more digits by increasing `n_mnist_classes` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "### For purpose of demonstration we're not going to use all of  ########\n",
    "### the digits. Increase n_mnist classes if you want to try     ########\n",
    "### training your neural net on more types of digits.           ########\n",
    "########################################################################\n",
    "\n",
    "n_mnist_classes = 3\n",
    "\n",
    "########################################################################\n",
    "########################################################################\n",
    "\n",
    "# load_mnist_wrapper brings in the MNIST data with keras.datasets.mnist.load_data()\n",
    "# and does some useful preprocessing. Check out the first code cell of this notebook\n",
    "# to see how this works.\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = load_mnist_wrapper(n_classes=\n",
    "                                                                                  n_mnist_classes)\n",
    "    \n",
    "# Show a few randomly selected images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(8,8))\n",
    "\n",
    "c = np.random.choice(X_train_mnist.shape[0], axes.size)\n",
    "digits, classes = X_train_mnist[c], y_train_mnist[c]\n",
    "\n",
    "for (ax,img,num) in zip(axes.flatten(), digits, classes):\n",
    "    ax.imshow(img.reshape((28,28)), cmap='gray')\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_xlabel(\"Digit: \" + str(num.argmax()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could apply a plain feedforward neural network to this dataset, and you'd get decent results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils  import to_categorical\n",
    "from keras        import backend\n",
    "\n",
    "# Slight modification of architecture from the following example in the\n",
    "# Keras repository:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "mnist_model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    Conv2D(32, (3,3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(.25),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(.5),\n",
    "    Dense(n_mnist_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "mnist_model.compile(loss='categorical_crossentropy', optimizer='adadelta',\n",
    "                    metrics=['accuracy'])\n",
    "mnist_model.fit(X_train_mnist, y_train_mnist, verbose=1, batch_size=256,\n",
    "                epochs=1, validation_data=(X_test_mnist, y_test_mnist))\n",
    "\n",
    "score = mnist_model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print('Test loss: %.4f'     % score[0])\n",
    "print('Test accuracy: %.4f' % score[1])\n",
    "\n",
    "# If we didn't get 100% classification accuracy, show some images that we\n",
    "# misclassified\n",
    "predictions = mnist_model.predict(X_test_mnist).argmax(axis=1)\n",
    "mclf_idx    = (predictions != y_test_mnist.argmax(axis=1)).flatten()\n",
    "X_mclf      = X_test_mnist[mclf_idx]\n",
    "y_mclf      = y_test_mnist[mclf_idx]\n",
    "pred_mclf   = predictions[mclf_idx]\n",
    "\n",
    "print(\"On test set, misclassified %d out of %d\" % (X_mclf.shape[0], predictions.size))\n",
    "\n",
    "if X_mclf.shape[0] >= 3:\n",
    "    fig, axes   = plt.subplots(1,3,figsize=(8,8))\n",
    "    c           = np.random.choice(X_mclf.shape[0], 3, replace=False)\n",
    "    for (ii,ax) in enumerate(axes):\n",
    "        ax.imshow(X_mclf[c[ii]].reshape((28,28)), cmap='gray')\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        pred_class, true_class = pred_mclf[c[ii]], y_mclf[c[ii]].argmax()\n",
    "        ax.set_xlabel(\"Predicted: \" + str(pred_class) + \n",
    "                      \"\\nTrue class: \" + str(true_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models <a id=\"saving\"></a>\n",
    "Neural networks are computationally expensive and require a large amount of time to train. The largest nets can take days to train on hundreds or thousands of specialized hardware units running in parallel. As a result, once you've trained a neural net, you may want to save it so that you can use it later on without having to re-train the network.\n",
    "\n",
    "Keras allows you to save an ANN to a file and then reload it later on with the `save()` function. When you call `model.save(filename)` Keras stores `model` in an HDF5 file with all the details of the network, including\n",
    "\n",
    "* the ANN's architecture (i.e. number of layers, nodes per layer, etc.);\n",
    "* the weights you found by running `model.fit`; and\n",
    "* the training configuration from `model.compile` (such as what loss function and optimizer you're using)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions ###################################\n",
    "def plot_circle_anns(X, classes, model, loaded_model):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(9,5))\n",
    "    for (ax,ann) in zip(axes,(model,loaded_model)):\n",
    "        ax.scatter(X[classes == 0,0], X[classes == 0,1]); ax.set_xticks([])\n",
    "        ax.scatter(X[classes == 1,0], X[classes == 1,1]); ax.set_yticks([])\n",
    "        plot_decision_boundary(X, ann, ax, incr=0.1)\n",
    "    axes[0].set_title(\"Original model\")\n",
    "    axes[1].set_title(\"Model loaded from file\")\n",
    "    plt.show()\n",
    "#########################################################\n",
    "\n",
    "from keras.models import load_model\n",
    "    \n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "\"\"\"\n",
    "Save the ANN as an HDF5 (.h5) file\n",
    "\"\"\"\n",
    "model.save('keras_circles_ann.h5')\n",
    "\n",
    "\"\"\"\n",
    "Now load model from disk\n",
    "\"\"\"\n",
    "loaded_model = load_model('keras_circles_ann.h5')\n",
    "\n",
    "\"\"\"\n",
    "Show decision boundaries for both ANNs side-by-side, overlaid\n",
    "on the dataset\n",
    "\"\"\"\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked correctly, the left and right plots should be identical.\n",
    "\n",
    "`model.save` will also save the current training state of the network. This allows you to spend time training a network with `model.fit`, save it, and then come back later and continue training the model. All you have to do is call the `fit` function again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do five more epochs of training with the model that was loaded\n",
    "# from the save file\n",
    "loaded_model.fit(X, classes, epochs=5, steps_per_epoch=512)\n",
    "\n",
    "plot_circle_anns(X, classes, model, loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to use `keras.models.load_model` is to share neural nets with others. For instance, I trained a neural net for the entire MNIST dataset (all 10 digits) using [some example code](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py) and added it to the GitHub repository for this workshop. The code cell below downloads this model from the repository (if it isn't already downloaded) and scores it on the MNIST test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "### This code is just to download the model off the ############\n",
    "### GitHub repository if you don't already have it  ############\n",
    "################################################################\n",
    "import shutil\n",
    "from urllib.request import urlopen\n",
    "\n",
    "DOWNLOAD_PATH = os.path.join(os.getcwd(), 'mnist_model.h5')\n",
    "REPO_PATH     = os.path.join(os.getcwd(), 'assets', 'models', 'mnist_model.h5')\n",
    "url           = 'https://github.com/wshand/Python-Data-Science-Workshop/blob/'\\\n",
    "                'master/assets/models/mnist_model.h5?raw=true'\n",
    "\n",
    "if not os.path.isfile(REPO_PATH) and not os.path.isfile(DOWNLOAD_PATH):\n",
    "    with urlopen(url) as response, open(DOWNLOAD_PATH, 'wb') as f:\n",
    "        print('Downloading model from', url)\n",
    "        print('Downloading to', DOWNLOAD_PATH)\n",
    "        shutil.copyfileobj(response, f)\n",
    "################################################################\n",
    "\n",
    "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = load_mnist_wrapper(n_classes=10)\n",
    "if os.path.isfile(REPO_PATH):\n",
    "    model = load_model(REPO_PATH)\n",
    "else:\n",
    "    model = load_model(DOWNLOAD_PATH)\n",
    "\n",
    "score = model.evaluate(X_test_mnist, y_test_mnist, verbose=0)\n",
    "print('Test loss (all 10 digits): %.4f'     % score[0])\n",
    "print('Test accuracy (all 10 digits): %.4f' % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing networks with TensorBoard <a id=\"tensorboard\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\"\"\"\n",
    "Clear TensorFlow models that we've made so far\n",
    "\"\"\"\n",
    "backend.clear_session()\n",
    "\n",
    "\"\"\"\n",
    "Create directory for storing TensorBoard log files, if it doesn't exist already.\n",
    "If it does, clear all logs currently in the directory.\n",
    "\"\"\"\n",
    "LOG_DIR=os.path.join(os.getcwd(), 'ann_keras_log_dir')\n",
    "print(\"Using\", LOG_DIR, \"as directory to store TensorBoard logs\")\n",
    "if not os.path.isdir(LOG_DIR):\n",
    "    os.mkdir(LOG_DIR)\n",
    "\n",
    "for f in os.listdir(LOG_DIR):\n",
    "    file_path = os.path.join(LOG_DIR, f)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.unlink(file_path)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "\"\"\"\n",
    "Create circles data\n",
    "\"\"\"\n",
    "X, classes = make_circles(n_samples=200, factor=0.3, noise=0.1)\n",
    "\n",
    "\"\"\"\n",
    "Build neural network\n",
    "\"\"\"\n",
    "model = Sequential([\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "tboard = TensorBoard(log_dir=LOG_DIR, histogram_freq=0,\n",
    "                    write_graph=True, write_images=True)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\"\"\"\n",
    "Fit the model. Use a TensorBoard instance as a callback so that we can track\n",
    "training over time.\n",
    "\"\"\"\n",
    "model.fit(X, classes, epochs=5, steps_per_epoch=512, verbose=0, callbacks=[tboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start a TensorBoard server, we would usually use the `tensorboard` command in our terminal. So for instance, if `LOG_DIR='/home/ann_keras_log_dir'` in the code above, I would go to my terminal/command line and write\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir /home/ann_keras_log_dir\n",
    "```\n",
    "\n",
    "to start TensorBoard. For convenience, I've added some Python code below that will do this for you. Run the following code cell and then go visit http://localhost:6006 in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start TensorBoard server\n",
    "from tensorboard import program, default\n",
    "\n",
    "tb = program.TensorBoard(default.get_plugins(), program.get_default_assets_zip_provider())\n",
    "tb.configure(argv=[None, '--logdir', LOG_DIR])\n",
    "tb.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources <a id=\"additional-resources\"></a>\n",
    "* [Keras documentation](https://keras.io/)\n",
    "* [playground.tensorflow.org](https://playground.tensorflow.org/) allows you to experiment with some simple neural nets.\n",
    "* [Hacker's guide to Neural Networks](https://karpathy.github.io/neuralnets/)\n",
    "* [Neural Networks and Deep Learning (online book)](http://neuralnetworksanddeeplearning.com/)\n",
    "* [Deep Learning](http://www.deeplearningbook.org/) -- textbook by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, who are among the top researchers in neural nets\n",
    "* References for specific ANN architectures:\n",
    "  * Convolutional networks\n",
    "  * LSTMs\n",
    "    * [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
    "* [Practical recommendations for gradient-based training of deep architectures](https://arxiv.org/abs/1206.5533v2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workshop-Kernel",
   "language": "python",
   "name": "workshop-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}